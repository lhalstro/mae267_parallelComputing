9/24
hpc1 computing cluster
    8 nodes 16 cores for mae 267
        can use more nodes if not currently in use

on hpc1 file system
    /home/lhalstro/
    to secure your files (user read/write/execute, nothing for anyone else):
        cd /home
        chmod 700 lhalstro

SLURM is the queue system
Jacob has TechPLOT, FIELDVIEW

Homeworks:
    Print out source code
    Give answers to questions

9/28
Max fortran line length is 72, need to continue with &
    Max with continues is 132

doxygen --> see fortran code as a flow chart

make sure all floats have decimal points
    will lose points
integer powers are evaluated much faster than real powers
5x --> five spaces
I5 -->  integer right justified within five spaces
'implicit none' means you cant declare variables implicityly
    avoids implicit variable confusion
    have to declare all variables explicitly

counting loops
    DO icounter =  istart, iend, increment

restart a do loop from current line --> 'cycle'

logic tests are computationally expensive --> keep them out of loops

10/1
write 'unformatted' type files to reduce file size (compared to 'formatted')

faster to define array before compile so it doesnt have to do it during execution

fortran is column major order array(column,row)
    different than c++, etc

nice diagram of indexing through arrays vs. through grid, slide 23

10/6
***check for allocated arrrays (i.e slide 7)
    grading based on this

pointer --> reduced need to make COPIES of data, save memory
linked list --> each item linked to next
    good for arrays that you dont know what size they'll be
        (i.e. soring convergence histories, search results)

keep coding as simple as possible, limit classes
keep codes short --> put subroutines in separate files

project 1
xp, yp = x prime, y prime
run code on CLUSTER NODE (see lecture 1)
for residual history, only print first 10/last 10 iterations
time entire run, AND iteration loop
plot computational grid in techplot

10/8
debug check --> use analytic solution, calculate 1st/2nd order derivaties at
    each cell and compare
    Errors will occur due to grid strechting, debug on uniform grid

10/13
Thread Safety --> guarantee that a message will arrive exactly as it was sent
***clock function given by prof is different, uses MPI

10/15
use linked lists to store series of info that you dont know how long it will be
    i.e. residual history
*** save in PLOT3D formatted to show in report
*** only need first and last few lines of file, whole thing is too big
run in background:
    'nohup ./main > 'listing' & --> run on front end of hpc1
    'sbatch runscript.sh' --> run on compute nodes
        see hpc1 wiki for notes on slurm

10/20
do's and dont's of projects
    DO
        optimize compile -O3
        comment code
        in-line compiler option (put subroutines inline)
        whole array operations --> dont use do loops for element-wise opertations
        do j, then do i --> order is same as the pipe info is stored in
    DONT
        **put logic inside do loops
        call subroutines inside loops
        use debug compiler options --> very slow

**actually calculate the cross-product of areas, dont use xp, yp
**calculate projected areas/volumes ahead of time, only need to do once
**test out CFL, use max that doesnt make it unstable
**better to allocate an entire array instead of list of scalars.  so for derived data type, instead of mesh(i,j)%x do mesh%x(i,j)
    allows better vectorization
    **call kernal on a per block basis --> 'CALL KERNAL(variable you need)'
    blocks(N)%u(imax,jmax,variable)
    where blocks holds all data, N is block number, u is a container for all variables, ij are indicies for
multiply by inverse of number is faster than divide
    compiler optimization should do this but just in case

hw3:
calcpip bug --> status variable for sends and recieves needs to be an array, not a scalar
staggar sends and recievies
256 intervals, use 1, 2, 4, and 8 processors (divisible)

10/22

